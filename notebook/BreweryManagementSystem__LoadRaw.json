{
    "name": "BreweryManagementSystem__LoadRaw",
    "properties": {
        "folder": {
            "name": "BreweryManagementSystem"
        },
        "nbformat": 4,
        "nbformat_minor": 2,
        "sessionProperties": {
            "driverMemory": "28g",
            "driverCores": 4,
            "executorMemory": "28g",
            "executorCores": 4,
            "numExecutors": 2,
            "conf": {
                "spark.dynamicAllocation.enabled": "false",
                "spark.dynamicAllocation.minExecutors": "2",
                "spark.dynamicAllocation.maxExecutors": "2",
                "spark.autotune.trackingId": ""
            }
        },
        "metadata": {
            "saveOutput": true,
            "enableDebugMode": false,
            "kernelspec": {
                "name": "synapse_pyspark",
                "display_name": "python"
            },
            "language_info": {
                "name": "python"
            },
            "sessionKeepAliveTimeout": 30
        },
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# BreweryManagementSystem Raw Vault Transformations\n",
                    "\n",
                    "This script has been generated with DIG CLI, by `michaelwellner` at `2022-01-20 13:51:46`.\n",
                    "\n",
                    "\n",
                    "```\n",
                    "dig generate raw --source breweryaz --system BreweryManagementSystem\n",
                    "```"
                ]
            },
            {
                "cell_type": "code",
                "source": [
                    "# The following input variables might be set, when calling the Notebook via a pipeline.\n",
                    "# If not, they will be initialized with default values.\n",
                    "if 'load_date' not in dir():\n",
                    "    load_date = '2022-01-07T22:00:00Z'\n"
                ],
                "execution_count": null
            },
            {
                "cell_type": "code",
                "source": [
                    "#\n",
                    "# This is actual library code - To be moved to Python library.\n",
                    "#\n",   
                    "import pyspark.sql.functions as F\n",   
                    "import textwrap\n",   
                    "\n",   
                    "from delta.tables import *\n",   
                    "from pyspark.sql import DataFrame, Column\n",   
                    "from pyspark.sql.session import SparkSession\n",   
                    "from typing import List, Optional, Tuple\n",   
                    "\n",   
                    "\n",   
                    "class DataVaultFunctions:\n",   
                    "\n",   
                    "    @staticmethod\n",   
                    "    def hash(column_names: List[str]) -> Column:\n",   
                    "        \"\"\"\n",   
                    "        Calculates a MD5 hash of provided columns.\n",   
                    "\n",   
                    "        :param column_names - The columns which should be included in the hash.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        columns = list(map(lambda c: F.col(c), column_names))\n",   
                    "        return F.md5(F.concat_ws(',', *columns))\n",   
                    "\n",   
                    "    @staticmethod\n",   
                    "    def to_columns(column_names: List[str]) -> List[Column]:\n",   
                    "        \"\"\"\"\n",   
                    "        Convert a list of column names to DataFrame columns.\n",   
                    "\n",   
                    "        :param column_names - The list if column names.\n",   
                    "        \"\"\"\n",   
                    "        return list(map(lambda c: F.col(c), column_names))\n",   
                    "\n",   
                    "    @staticmethod\n",   
                    "    def to_timestamp(column_name: str = 'load_date', pattern: str = \"yyyy-MM-dd'T'HH:mm:ss'Z'\") -> Column:\n",   
                    "        \"\"\"\n",   
                    "        Converts a date str of a format (pattern) to a timestamp column.\n",   
                    "\n",   
                    "        :param column_name - The column which contains the date string.\n",   
                    "        :param pattern - The (java.time) pattern of the date string.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        return F.to_timestamp(F.col(column_name), pattern);\n",   
                    "\n",   
                    "\n",   
                    "class LoadRaw:\n",   
                    "\n",   
                    "    # The column prefix is added to data vault specific column names in the staging table.\n",   
                    "    DV_COLUMN_PREFIX = 'dv__'\n",   
                    "\n",   
                    "    # constants for common column names\n",   
                    "    HKEY = 'hkey'\n",   
                    "    HDIFF = 'hdiff'\n",   
                    "    LAST_SEEN_DATE = 'last_seen_date'\n",   
                    "    LOAD_DATE = 'load_date'\n",   
                    "    LOAD_END_DATE = 'load_end_date'\n",   
                    "    RECORD_SOURCE = 'record_source'\n",   
                    "\n",   
                    "    # constants for spark datatypes\n",   
                    "    STRING_TYPE = \"STRING\"\n",   
                    "    TIMESTAMP_TYPE = \"TIMESTAMP\"\n",   
                    "\n",   
                    "\n",   
                    "    def __init__(\n",   
                    "        self, spark: SparkSession, load_date: str, \n",   
                    "        source_system_name: str, \n",   
                    "        source_system_short_name: str,\n",   
                    "        source_base_path: str, \n",   
                    "        staging_database_path: str,\n",   
                    "        raw_database_path: str,\n",   
                    "        verbose: bool = False) -> None:\n",   
                    "        \"\"\"\n",   
                    "        :param spark - The spark session to use for running transformations.\n",   
                    "        :param load_date - The load date/time which is used during the load.\n",   
                    "        :param source_system_name - The name of the source system.\n",   
                    "        :param source_system_short_name - An abbreviation of the name, used for naming resources. MUST be snake case.\n",   
                    "\n",   
                    "        :param source_base_path - The base path for source files on the data lake (w/o trailing slash).\n",   
                    "        :param staging_database_path - The location of the staging database on the data lake.\n",   
                    "        :param raw_database_path - The location of the raw database on the data lake.\n",   
                    "\n",   
                    "        :param verbose - If True, result tables will be printed to output.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        self.spark = spark\n",   
                    "        self.load_date = load_date\n",   
                    "        self.source_system_name = source_system_name\n",   
                    "        self.source_system_short_name = source_system_short_name\n",   
                    "\n",   
                    "        self.source_base_path = source_base_path\n",   
                    "        self.staging_database_path = staging_database_path\n",   
                    "        self.raw_database_path = raw_database_path\n",   
                    "\n",   
                    "        self.verbose = verbose\n",   
                    "\n",   
                    "        #\n",   
                    "        # Calculated values\n",   
                    "        #\n",   
                    "        self.staging_database_name = f'{self.source_system_short_name}__staging'\n",   
                    "        self.raw_database_name = f'{self.source_system_short_name}__raw'\n",   
                    "\n",   
                    "    def create_hub(self, name: str, business_key_columns: List[Tuple[str, str]]) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Creates a hub table in the raw database. Does only create the table if it does not exist yet.\n",   
                    "\n",   
                    "        :param name - The name of the hub table, usually starting with `HUB__`.\n",   
                    "        :param business_key_columns - The columns for the hub are the keys which compose the business key. Tuple contains (name, type).\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        columns: List[Tuple[str, str, bool]] = [\n",   
                    "            (self.HKEY, self.STRING_TYPE, False),\n",   
                    "            (self.LOAD_DATE, self.TIMESTAMP_TYPE, False),\n",   
                    "            (self.LAST_SEEN_DATE, self.TIMESTAMP_TYPE, False),\n",   
                    "            (self.RECORD_SOURCE, self.STRING_TYPE, False)\n",   
                    "        ]\n",   
                    "\n",   
                    "        for col in business_key_columns:\n",   
                    "            columns.append((col[0], col[1], True))\n",   
                    "\n",   
                    "        self.__create_table(name, columns)\n",   
                    "\n",   
                    "    def create_link(self, name: str, foreign_hash_key_columns: List[str]) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Creates a link table in the raw database. Does only create the table if it does not exist yet.\n",   
                    "\n",   
                    "        :param name - The name of the link table, usually starting with `LNK__`.\n",   
                    "        :param foreign_hash_key_columns - The name of the hash key columns pointing to other hubs.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        columns: List[Tuple[str, str, bool]] = [\n",   
                    "            (self.HKEY, self.STRING_TYPE, False),\n",   
                    "            (self.LOAD_DATE, self.TIMESTAMP_TYPE, False),\n",   
                    "            (self.LAST_SEEN_DATE, self.TIMESTAMP_TYPE, False),\n",   
                    "            (self.RECORD_SOURCE, self.STRING_TYPE, False)\n",   
                    "        ]\n",   
                    "\n",   
                    "        for col in foreign_hash_key_columns:\n",   
                    "            columns.append((col, self.STRING_TYPE, True))\n",   
                    "\n",   
                    "        self.__create_table(name, columns)\n",   
                    "\n",   
                    "    def create_pit(self, name: str, satellite_names: List[str]) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Creates a point-in-time table for a hub.\n",   
                    "\n",   
                    "        :param name - The name of the PIT Table, usually `SAT__{ENTITY_NAME}__PIT`.\n",   
                    "        :param satellite_columns - The name of the satellite tables which belong to the entity.\n",   
                    "        \"\"\"\n",   
                    "        columns: List[Tuple[str, str, bool]] = [\n",   
                    "            (self.HKEY, self.STRING_TYPE, False),\n",   
                    "            (self.HDIFF, self.STRING_TYPE, False),\n",   
                    "            (self.LOAD_DATE, self.TIMESTAMP_TYPE, False),\n",   
                    "            (self.LOAD_END_DATE, self.TIMESTAMP_TYPE, True),\n",   
                    "        ]\n",   
                    "\n",   
                    "        for sat in satellite_names:\n",   
                    "            columns.append((sat.lower().replace('sat__', ''), self.TIMESTAMP_TYPE, True))\n",   
                    "\n",   
                    "        self.__create_table(name, columns)\n",   
                    "\n",   
                    "    def create_satellite(self, name: str, attribute_columns: List[Tuple[str, str]]) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Creates a satellite table in the raw database. Does only create the table if it does not exist yet.\n",   
                    "\n",   
                    "        :param name - The name of the satellite table, usually starting with `SAT__`.\n",   
                    "        :param attribute_columns - The attributes which should be stored in the satellite.\n",   
                    "            :0 The name of the column\n",   
                    "            :1 The Spark SQL type of the column.\n",   
                    "        \"\"\"\n",   
                    "        columns: List[Tuple[str, str, bool]] = [\n",   
                    "            (self.HKEY, self.STRING_TYPE, False),\n",   
                    "            (self.HDIFF, self.STRING_TYPE, False),\n",   
                    "            (self.LOAD_DATE, self.TIMESTAMP_TYPE, False),\n",   
                    "            (self.LOAD_END_DATE, self.TIMESTAMP_TYPE, True)\n",   
                    "        ]\n",   
                    "\n",   
                    "        for col in attribute_columns: \n",   
                    "            columns.append((col[0], col[1], True))\n",   
                    "\n",   
                    "        self.__create_table(name, columns)        \n",   
                    "\n",   
                    "    def initialize_database(self) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Initialize database.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        self.spark.sql(f\"\"\"CREATE DATABASE IF NOT EXISTS {self.staging_database_name} LOCATION '{self.staging_database_path}'\"\"\")\n",   
                    "        self.spark.sql(f\"\"\"CREATE DATABASE IF NOT EXISTS {self.raw_database_name} LOCATION '{self.raw_database_path}'\"\"\")\n",   
                    "\n",   
                    "    def load_hub_from_staging_table(self, staging: str, hub: str, business_key_column_names: List[str], satellites: List[Tuple[str, List[str]]]) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Loads a hub from a staging table. The staging table must have a HKEY calculated.\n",   
                    "\n",   
                    "        :param staging - The name of the staging table.\n",   
                    "        :param hub - The name of the hub table in the raw vault.\n",   
                    "        :param business_key_column_names - The list of columns which contribute to the business key of the hub.\n",   
                    "        :param satellites - Optional. A list of satellites which is loaded from the link. The form of the tuple is:\n",   
                    "            :0 - The name of the satellite table to load.\n",   
                    "            :1 - The list of attributes/ columns which should be stored in the satellite.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        #\n",   
                    "        # Select relevant columns from staging table.\n",   
                    "        #\n",   
                    "        common_column_names = [self.HKEY, self.LOAD_DATE, self.LAST_SEEN_DATE, self.RECORD_SOURCE]\n",   
                    "        common_column_names_with_prefix = list(map(lambda s: f'{self.DV_COLUMN_PREFIX}{s}', common_column_names))\n",   
                    "        all_column_names = common_column_names_with_prefix + business_key_column_names\n",   
                    "        all_columns = DataVaultFunctions.to_columns(all_column_names)\n",   
                    "\n",   
                    "        df = self.spark.sql(f'SELECT * FROM {self.staging_database_name}.{staging}').select(all_columns)\n",   
                    "        \n",   
                    "        for column in common_column_names:\n",   
                    "            df = df.withColumnRenamed(f'{self.DV_COLUMN_PREFIX}{column}', column)\n",   
                    "\n",   
                    "        #\n",   
                    "        # Merge new data with existing data in Hub.\n",   
                    "        #\n",   
                    "        self.__update_hub(hub, df)\n",   
                    "\n",   
                    "        #\n",   
                    "        # Load satellites.\n",   
                    "        #\n",   
                    "        for sat in satellites:\n",   
                    "            self.load_satellite_from_staging_table(staging, sat[0], sat[1])\n",   
                    "\n",   
                    "    def load_link_from_link_staging_table(self, staging: str, staging_to: List[Tuple[str, str, str, str]], link: str, satellites: List[Tuple[str, List[str]]]) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Loads a link with data from a staging table which is a already a link table in the source.\n",   
                    "\n",   
                    "        :param staging - The name of the staging table which contains two or more foreign keys.\n",   
                    "        :param staging_to - Information about the linked tables/ hubs.\n",   
                    "            :0 The name of the linked staging table\n",   
                    "            :1 The name of the FK column in the source staging table\n",   
                    "            :2 The name of the FK column in the linked staging table\n",   
                    "            :3 The name of the hkey column in the link table\n",   
                    "        :param link - The name of the link table to load.\n",   
                    "        :param satellites - Optional. A list of satellites which is loaded from the link. The form of the tuple is:\n",   
                    "            :0 - The name of the satellite table to load.\n",   
                    "            :1 - The list of attributes/ columns which should be stored in the satellite.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        staging_to: List[Tuple[int, Tuple[str, str, str, str]]] = list(enumerate(staging_to))\n",   
                    "        hkey_selectors = list(map(lambda t: f't{t[0]}.{self.DV_COLUMN_PREFIX}{self.HKEY} AS {t[1][3]}', staging_to))\n",   
                    "        hkey_columns = list(map(lambda t: t[1][3], staging_to))\n",   
                    "        \n",   
                    "        attr_selectors: List[str] = []\n",   
                    "        for sat in satellites:\n",   
                    "            for attr in sat[1]:\n",   
                    "                attr_selectors.append(f'f.{attr} AS {attr}')\n",   
                    "        \n",   
                    "        selectors = hkey_selectors + attr_selectors\n",   
                    "        selectors = ', '.join(selectors)\n",   
                    "\n",   
                    "        joins = list(map(lambda t: f'JOIN {self.staging_database_name}.{t[1][0]} AS t{t[0]} ON f.{t[1][1]} = t{t[0]}.{t[1][2]}', staging_to))\n",   
                    "        joins = ' '.join(joins)\n",   
                    "\n",   
                    "        query = f'SELECT DISTINCT {selectors} FROM {self.staging_database_name}.{staging} AS f {joins}'\n",   
                    "        self.load_link_from_query(query, link, hkey_columns, satellites)\n",   
                    "\n",   
                    "    def load_link_from_linked_staging_tables(self, staging_from: str, staging_to: str, fk_column_from: str, fk_column_to: str, hkey_from: str, hkey_to: str, link: str) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Loads a link with data from two directly linked tables. \n",   
                    "\n",   
                    "        :param staging_from - The staging table name for the first table.\n",   
                    "        :param staging_to - The staging table name for the second table.\n",   
                    "        :param fk_column_from - The column of the foreign key in the first staging table.\n",   
                    "        :param fk_column_to - The column in the second table where the foreign key of the first table points to.\n",   
                    "        :param hkey_from - The name of the first hash key column in the link table.\n",   
                    "        :param hkey_to - The name of the second hash key column in the table.\n",   
                    "        :param link - The name of the link table to load.\n",   
                    "        :param \n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        query = textwrap.dedent(f\"\"\"\n",   
                    "            SELECT DISTINCT f.{self.DV_COLUMN_PREFIX}{self.HKEY} AS {hkey_from}, t.{self.DV_COLUMN_PREFIX}{self.HKEY} as {hkey_to}\n",   
                    "            FROM {self.staging_database_name}.{staging_from} AS f\n",   
                    "            JOIN {self.staging_database_name}.{staging_to} AS t ON f.{fk_column_from} = t.{fk_column_to}\n",   
                    "            \"\"\".strip())\n",   
                    "\n",   
                    "        self.load_link_from_query(query, link, [hkey_from, hkey_to], [])\n",   
                    "\n",   
                    "    def load_link_from_query(self, query: str, link: str, hkey_columns: List[str], satellites: List[Tuple[str, List[str]]]):\n",   
                    "        \"\"\"\n",   
                    "        Loads link data from a query which joins data from staging. The function calculates the HKEY for the link based on `hkey_columns`.\n",   
                    "        Additionally it also loads the satellites of the link.\n",   
                    "\n",   
                    "        :param query - The Spark SQL query to fetch/ join the data.\n",   
                    "        :param link - The name of the link table to load.\n",   
                    "        :param hkey_columns - The list of columns which are included in the link, usually they have the form `{hub_name}__hkey`.\n",   
                    "        :param satellites - Optional. A list of satellites which is loaded from the link. The form of the tuple is:\n",   
                    "            :0 - The name of the satellite table to load.\n",   
                    "            :1 - The list of attributes/ columns which should be stored in the satellite.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        df = self.spark.sql(query)\n",   
                    "        self.load_link_from_df(df, link, hkey_columns, satellites)\n",   
                    "            \n",   
                    "    def load_link_from_df(self, df: DataFrame, link: str, hkey_columns: List[str], satellites: List[Tuple[str, List[str]]]):\n",   
                    "        \"\"\"\n",   
                    "        Loads link data from a dataframe with already joined data from staging. The function calculates the HKEY for the link based on `hkey_columns`.\n",   
                    "        Additionally it also loads the satellites of the link.\n",   
                    "\n",   
                    "        :param df - The dataframe containing the joined data.\n",   
                    "        :param link - The name of the link table to load.\n",   
                    "        :param hkey_columns - The list of columns which are included in the link, usually they have the form `{hub_name}__hkey`.\n",   
                    "        :param satellites - Optional. A list of satellites which is loaded from the link. The form of the tuple is:\n",   
                    "            :0 - The name of the satellite table to load.\n",   
                    "            :1 - The list of attributes/ columns which should be stored in the satellite.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        #\n",   
                    "        # Load link table\n",   
                    "        #\n",   
                    "        common_column_names = [self.HKEY, self.LOAD_DATE, self.LAST_SEEN_DATE, self.RECORD_SOURCE]\n",   
                    "        all_column_names = common_column_names + hkey_columns\n",   
                    "        all_columns = DataVaultFunctions.to_columns(all_column_names)\n",   
                    "\n",   
                    "        df = df \\\n",   
                    "            .withColumn(self.HKEY, DataVaultFunctions.hash(hkey_columns)) \\\n",   
                    "            .withColumn(f'{self.LOAD_DATE}', F.lit(self.load_date)) \\\n",   
                    "            .withColumn(f'{self.LAST_SEEN_DATE}', DataVaultFunctions.to_timestamp()) \\\n",   
                    "            .withColumn(f'{self.LOAD_DATE}', DataVaultFunctions.to_timestamp()) \\\n",   
                    "            .withColumn(f'{self.RECORD_SOURCE}', F.lit(self.source_system_name)) \\\n",   
                    "            .select(all_columns)\n",   
                    "\n",   
                    "        self.__update_link(link, df)\n",   
                    "\n",   
                    "        #\n",   
                    "        # Load satellites.\n",   
                    "        #\n",   
                    "        for sat in satellites:\n",   
                    "            self.load_satellite_from_df(df, sat[0], sat[1])\n",   
                    "\n",   
                    "    def load_satellite_from_staging_table(self, staging: str, sat: str, attribute_column_names: List[str]):\n",   
                    "        \"\"\"\n",   
                    "        Loads a satellite from a staging table. The staging table must have a HKEY calculated.\n",   
                    "\n",   
                    "        :param staging - The name of the staging table.\n",   
                    "        :param sat - The name of the satellite table.\n",   
                    "        :param attribute_column_names - The list of columns which should be included in the satellite.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        query = f'SELECT * FROM {self.staging_database_name}.{staging}'\n",   
                    "        self.load_satellite_from_query(query, sat, attribute_column_names)\n",   
                    "\n",   
                    "    def load_satellite_from_linked_staging_table(self, staging_root: Tuple[str, str], staging_attributes: Tuple[str, str], sat: str, attribute_column_names: List[str]):\n",   
                    "        \"\"\"\n",   
                    "        Loads a satellite from two tables. One containing the entity incl. the hash key (staging_root) and the other table contains the\n",   
                    "        satellites attributes (staging_attributes).\n",   
                    "\n",   
                    "        :param staging_root - A staging table name which contains the HKey for the satellite.\n",   
                    "            :0 The name of the staging table\n",   
                    "            :1 The foreign column name\n",   
                    "        :param staging_attributes - A staging table which contains the attributes of the sattelite.\n",   
                    "            :0 The name of the staging table\n",   
                    "            :1 The name of the column name which contains the FK to the staging_root table\n",   
                    "        :param sat - The name of the satellite table.\n",   
                    "        :param attribute_column_names - The list of columns which should be included in the satellite.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "\n",   
                    "        selectors = [f'r.{self.DV_COLUMN_PREFIX}{self.HKEY}', f'r.{self.DV_COLUMN_PREFIX}{self.LOAD_DATE}']\n",   
                    "        selectors = selectors + list(map(lambda c: f'a.{c}', attribute_column_names))\n",   
                    "        selectors = ', '.join(selectors)\n",   
                    "\n",   
                    "        query = f\"\"\"\n",   
                    "            SELECT {selectors} FROM {self.staging_database_name}.{staging_attributes[0]} AS a JOIN {self.staging_database_name}.{staging_root[0]} AS r ON a.{staging_attributes[1]} = r.{staging_root[1]}\n",   
                    "            \"\"\".strip();\n",   
                    "\n",   
                    "        self.load_satellite_from_query(query, sat, attribute_column_names)\n",   
                    "\n",   
                    "    def load_satellite_from_query(self, query: str, sat: str, attribute_column_names: List[str], hkey_column_names: Optional[List[str]] = None):\n",   
                    "        \"\"\"\n",   
                    "        Loads a satellite from a data framed which is loaded with the query.\n",   
                    "\n",   
                    "        :param query - The query to fetch the source date of the satellite.\n",   
                    "        :param sat - The name of the satellite table.\n",   
                    "        :param attribute_column_names - The attributes which should be inserted in the satellite.\n",   
                    "        :param hkey_column_names - Optional. If provided, an HKEY will be calculated based on these columns.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        df = self.spark.sql(query)\n",   
                    "        self.load_satellite_from_df(df, sat, attribute_column_names, hkey_column_names)\n",   
                    "\n",   
                    "    def load_satellite_from_df(self, df: DataFrame, sat: str, attribute_column_names: List[str], hkey_column_names: Optional[List[str]] = None):\n",   
                    "        \"\"\"\n",   
                    "        Loads a satellite from a data framed which is loaded with the query.\n",   
                    "\n",   
                    "        :param df - The data frame which is acts as input for the satellite.\n",   
                    "        :param sat - The name of the satellite table.\n",   
                    "        :param attribute_column_names - The attributes which should be inserted in the satellite.\n",   
                    "        :param hkey_column_names - Optional. If provided, an HKEY will be calculated based on these columns.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        #\n",   
                    "        # Select relevant columns from staging table.\n",   
                    "        #\n",   
                    "        common_column_names = [self.HKEY, self.HDIFF, self.LOAD_DATE, self.LOAD_END_DATE]\n",   
                    "        all_column_names = common_column_names + attribute_column_names\n",   
                    "        all_columns = DataVaultFunctions.to_columns(all_column_names)\n",   
                    "\n",   
                    "        for column in [self.HKEY, self.LOAD_DATE]:\n",   
                    "            df = df.withColumnRenamed(f'{self.DV_COLUMN_PREFIX}{column}', column)\n",   
                    "\n",   
                    "        if hkey_column_names is not None:\n",   
                    "            df = df.withColumn(self.HKEY, DataVaultFunctions.hash(hkey_column_names))\n",   
                    "\n",   
                    "        df = df \\\n",   
                    "            .withColumn(self.HDIFF, DataVaultFunctions.hash(attribute_column_names)) \\\n",   
                    "            .withColumn(self.LOAD_END_DATE, F.lit(None)) \\\n",   
                    "            .select(all_columns)\n",   
                    "\n",   
                    "        #\n",   
                    "        # Merge new data with existing data in Satellite.\n",   
                    "        #\n",   
                    "        self.__update_satellite(sat, df)\n",   
                    "\n",   
                    "    def stage_table(self, name: str, source: str, hkey_columns: List[str] = []) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Stages a source table. Additional columns will be created/ calculated and stored in the staging database. \n",   
                    "\n",   
                    "        :param name - The name of the staged table.\n",   
                    "        :param source - The source file path, relative to source_base_path (w/o leading slash).\n",   
                    "        :param hkey_columns - Optional. Column names which should be used to calculate a hash key.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        #\n",   
                    "        # Load source data from Parquet file.\n",   
                    "        #\n",   
                    "        df = self.spark.read.load(f'{self.source_base_path}/{source}', format='parquet')\n",   
                    "\n",   
                    "        #\n",   
                    "        # Add DataVault specific columns\n",   
                    "        #\n",   
                    "        df = df \\\n",   
                    "            .withColumn(f'{self.DV_COLUMN_PREFIX}{self.LOAD_DATE}', F.lit(self.load_date)) \\\n",   
                    "            .withColumn(f'{self.DV_COLUMN_PREFIX}{self.LAST_SEEN_DATE}', DataVaultFunctions.to_timestamp(f'{self.DV_COLUMN_PREFIX}{self.LOAD_DATE}')) \\\n",   
                    "            .withColumn(f'{self.DV_COLUMN_PREFIX}{self.LOAD_DATE}', DataVaultFunctions.to_timestamp(f'{self.DV_COLUMN_PREFIX}{self.LOAD_DATE}')) \\\n",   
                    "            .withColumn(f'{self.DV_COLUMN_PREFIX}{self.RECORD_SOURCE}', F.lit(self.source_system_name))\n",   
                    "\n",   
                    "        if len(hkey_columns) > 0: df = df.withColumn(f'{self.DV_COLUMN_PREFIX}{self.HKEY}', DataVaultFunctions.hash(hkey_columns))\n",   
                    "\n",   
                    "        #\n",   
                    "        # Write staged table into staging area.\n",   
                    "        #\n",   
                    "        df.write.mode('overwrite').saveAsTable(f'{self.staging_database_name}.{name}')\n",   
                    "\n",   
                    "    def __create_table(self, name: str, columns: List[Tuple[str, str, bool]]):\n",   
                    "        \"\"\"\n",   
                    "        Helper function to create a Delta Table in raw vault (if it does not exist yet).\n",   
                    "\n",   
                    "        :param name - The name of the table.\n",   
                    "        :param columns - The columns of the table (name, spark type, nullable).\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        cmd = DeltaTable.createIfNotExists(self.spark).tableName(f'{self.raw_database_name}.{name}')\n",   
                    "\n",   
                    "        for col in columns:\n",   
                    "            cmd = cmd.addColumn(col[0], col[1], nullable=col[2])\n",   
                    "\n",   
                    "        cmd.execute()\n",   
                    "\n",   
                    "    def update_pit(self, name: str, satellite_names: List[str]) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Updates a PIT table after data has been loaded into satellites.\n",   
                    "\n",   
                    "        :param name - The name of the PIT Table, usually `SAT__{ENTITY_NAME}__PIT`.\n",   
                    "        :param satellite_columns - The name of the satellite tables which belong to the entity.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        def get_sat_name(s: str) -> str:\n",   
                    "            return s.lower().replace('sat__', '')\n",   
                    "\n",   
                    "\n",   
                    "        sat_col_names: List[str] = list(map(lambda s: get_sat_name(s), satellite_names))\n",   
                    "        all_column_names: List[str] = [self.HKEY, self.HDIFF, self.LOAD_DATE, self.LOAD_END_DATE]\n",   
                    "        all_column_names = all_column_names + sat_col_names\n",   
                    "\n",   
                    "        df: Optional[DataFrame] = None\n",   
                    "\n",   
                    "        for sat in satellite_names:\n",   
                    "            table = f'{self.raw_database_name}.{sat}'\n",   
                    "            sat_name = get_sat_name(sat)\n",   
                    "            sat_load_date = f'{sat_name}'\n",   
                    "\n",   
                    "            sat_df: DataFrame = DeltaTable \\\n",   
                    "                .forName(self.spark, table) \\\n",   
                    "                .toDF() \\\n",   
                    "                .withColumnRenamed(self.LOAD_DATE, sat_load_date) \\\n",   
                    "                .select(self.HKEY, sat_load_date)\n",   
                    "\n",   
                    "            if df is None:\n",   
                    "                df = sat_df.withColumn(self.LOAD_DATE, F.lit(self.load_date))\n",   
                    "            else:\n",   
                    "                df = df.join(sat_df, df[self.HKEY] == sat_df[self.HKEY])\n",   
                    "\n",   
                    "        df = df \\\n",   
                    "            .withColumn(self.HDIFF, DataVaultFunctions.hash(sat_col_names)) \\\n",   
                    "            .withColumn(self.LOAD_END_DATE, F.lit(None)) \\\n",   
                    "            .select(all_column_names)\n",   
                    "\n",   
                    "        #\n",   
                    "        # Merge new data with existing data in PIT.\n",   
                    "        #\n",   
                    "        self.__update_satellite(name, df)\n",   
                    "\n",   
                    "\n",   
                    "    def __update_hub(self, name: str, df: DataFrame) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Update HUB with new data loaded from staging.\n",   
                    "\n",   
                    "        :param name - The name of the hub table in the raw database.\n",   
                    "        :param data - The prepared data. The DataFrame schema must match the Hub table schema.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        table = f'{self.raw_database_name}.{name}'\n",   
                    "        map_columns = dict(list(map(lambda c: (c, f'updates.{c}'), df.columns)))\n",   
                    "\n",   
                    "        DeltaTable.forName(self.spark, table) \\\n",   
                    "            .alias('hub') \\\n",   
                    "            .merge(df.alias('updates'), f'hub.{self.HKEY} = updates.{self.HKEY}') \\\n",   
                    "            .whenMatchedUpdate(set = { self.LAST_SEEN_DATE: f'updates.{self.LAST_SEEN_DATE}' }) \\\n",   
                    "            .whenNotMatchedInsert(values = map_columns) \\\n",   
                    "            .execute()\n",   
                    "\n",   
                    "        if self.verbose:\n",   
                    "            print(textwrap.dedent(f\"\"\"\n",   
                    "                #\n",   
                    "                # Updated `{table}`\n",   
                    "                #\n",   
                    "                \"\"\").strip())\n",   
                    "\n",   
                    "            self.spark.sql(f'SELECT * FROM {table} ORDER BY {self.LOAD_DATE}').show()\n",   
                    "\n",   
                    "    def __update_link(self, name: str, df: DataFrame) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Update Link with new data loaded from staging.\n",   
                    "\n",   
                    "        :param name - The name of the link table in the raw database.\n",   
                    "        :param data - The perpared data. The DataFrame schema must match the Link table schema.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        table = f'{self.raw_database_name}.{name}'\n",   
                    "        map_columns = dict(list(map(lambda c: (c, f'updates.{c}'), df.columns)))\n",   
                    "\n",   
                    "        DeltaTable.forName(self.spark, table) \\\n",   
                    "            .alias('hub') \\\n",   
                    "            .merge(df.alias('updates'), f'hub.{self.HKEY} = updates.{self.HKEY}') \\\n",   
                    "            .whenMatchedUpdate(set = { self.LAST_SEEN_DATE: f'updates.{self.LAST_SEEN_DATE}' }) \\\n",   
                    "            .whenNotMatchedInsert(values = map_columns) \\\n",   
                    "            .execute()\n",   
                    "\n",   
                    "        if self.verbose:\n",   
                    "            print(textwrap.dedent(f\"\"\"\n",   
                    "                #\n",   
                    "                # Updated `{table}`\n",   
                    "                #\n",   
                    "                \"\"\").strip())\n",   
                    "\n",   
                    "            self.spark.sql(f'SELECT * FROM {table} ORDER BY {self.LOAD_DATE}').show()\n",   
                    "\n",   
                    "\n",   
                    "    def __update_satellite(self, name: str, df: DataFrame) -> None:\n",   
                    "        \"\"\"\n",   
                    "        Update Satellite with new data loaded from staging.\n",   
                    "\n",   
                    "        :param name - The name of the satellite table in the raw database.\n",   
                    "        :param data - The prepared data. The DataFrame schema must match the Satellite table schema.\n",   
                    "        \"\"\"\n",   
                    "\n",   
                    "        table = f'{self.raw_database_name}.{name}'\n",   
                    "        map_columns = dict(list(map(lambda c: (c, f'updates.{c}'), df.columns)))\n",   
                    "\n",   
                    "        DeltaTable.forName(self.spark, table) \\\n",   
                    "            .alias('sat') \\\n",   
                    "            .merge(df.alias('updates'), f'sat.{self.HKEY} = updates.{self.HKEY} and sat.{self.HDIFF} = updates.{self.HDIFF}') \\\n",   
                    "            .whenNotMatchedInsert(values = map_columns) \\\n",   
                    "            .execute()\n",   
                    "\n",   
                    "        dfUpdated = self.spark.sql(f'''\n",   
                    "            SELECT l.{self.HKEY}, l.{self.HDIFF}, r.{self.LOAD_DATE} FROM {table} AS l \n",   
                    "            FULL OUTER JOIN {table} AS r ON l.{self.HKEY} = r.{self.HKEY}\n",   
                    "            WHERE l.{self.LOAD_END_DATE} IS NULL\n",   
                    "            AND l.{self.HDIFF} != r.{self.HDIFF}\n",   
                    "            AND l.{self.LOAD_DATE} < r.{self.LOAD_DATE}\n",   
                    "        ''')\n",   
                    "\n",   
                    "        DeltaTable.forName(self.spark, table) \\\n",   
                    "            .alias('sat') \\\n",   
                    "            .merge(dfUpdated.alias('updates'), f'sat.{self.HKEY} = updates.{self.HKEY} and sat.{self.HDIFF} = updates.{self.HDIFF}') \\\n",   
                    "            .whenMatchedUpdate(set = { 'load_end_date': 'updates.load_date' }) \\\n",   
                    "            .execute()\n",   
                    "\n",   
                    "        if self.verbose:\n",   
                    "            print(textwrap.dedent(f\"\"\"\n",   
                    "                #\n",   
                    "                # Updated `{table}`\n",   
                    "                #\n",   
                    "                \"\"\").strip())\n",   
                    "\n",   
                    "            self.spark.sql(f'SELECT * FROM {table} ORDER BY {self.LOAD_DATE}').show()\n",   
                    "\n"
                ],
                "execution_count": null
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "source_system_name = 'BreweryManagementSystem'\n",
                    "source_system_short_name = 'bms'\n",
                    "source_base_path = f'abfss://staging@samwbrewerydatalake.dfs.core.windows.net/BreweryManagementSystem/{load_date[:4]}/{load_date[5:7]}/{load_date[8:10]}'\n",
                    "staging_database_path = 'abfss://staging@samwbrewerydatalake.dfs.core.windows.net/BreweryManagementSystem/current'\n",
                    "raw_database_path = 'abfss://raw@samwbrewerydatalake.dfs.core.windows.net/BreweryManagementSystem'\n",
                    "verbose = False\n",
                    "\n",
                    "load = LoadRaw(spark, load_date, source_system_name, source_system_short_name, source_base_path, staging_database_path, raw_database_path, verbose)"
                ],
                "execution_count": null
            },
            {
                "cell_type": "code",
                "source": [   
                    "#\n",   
                    "# Initialize tables\n",   
                    "#\n",   
                    "load.create_hub(\n",   
                    "    'HUB__MA_EMPLOYEES',\n",   
                    "    [\n",   
                    "        ('id', 'STRING')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PD_EXPERIMENTS',\n",   
                    "    [\n",   
                    "        ('brewer', 'STRING'), \n",   
                    "        ('description', 'STRING')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PD_EXTERNAL_RATING_EVENTS',\n",   
                    "    [\n",   
                    "        ('year', 'INT'), \n",   
                    "        ('month', 'INT')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PD_INGREDIENTS',\n",   
                    "    [\n",   
                    "        ('id', 'INT')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PD_INTERNAL_RATINGS',\n",   
                    "    [\n",   
                    "        ('experiment_id', 'INT'), \n",   
                    "        ('rated_by', 'STRING')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_BOTTLINGS',\n",   
                    "    [\n",   
                    "        ('id', 'INT')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_BREWS',\n",   
                    "    [\n",   
                    "        ('id', 'INT')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_BREWS_BOILINGS',\n",   
                    "    [\n",   
                    "        ('brew_id', 'INT'), \n",   
                    "        ('start_time', 'STRING')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_BREWS_MASHINGS',\n",   
                    "    [\n",   
                    "        ('brew_id', 'INT'), \n",   
                    "        ('start_time', 'STRING')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_BREWS_MASHING_RESTS',\n",   
                    "    [\n",   
                    "        ('brew_id', 'INT'), \n",   
                    "        ('start_time', 'STRING')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_BREWS_SPARGINGS',\n",   
                    "    [\n",   
                    "        ('brew_id', 'INT'), \n",   
                    "        ('start_time', 'STRING')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_INGREDIENTS',\n",   
                    "    [\n",   
                    "        ('id', 'INT')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_INGREDIENT_PRODUCTS',\n",   
                    "    [\n",   
                    "        ('id', 'INT')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_RECIPES',\n",   
                    "    [\n",   
                    "        ('beer_id', 'STRING')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_hub(\n",   
                    "    'HUB__PROD_RECIPES_INSTRUCTIONS',\n",   
                    "    [\n",   
                    "        ('id', 'INT')\n",   
                    "    ])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PD_EXTERNAL_RATING_EVENTS__CANDIDATE_1__PD_EXPERIMENTS', \n",   
                    "    ['pd_external_rating_events__hkey', 'pd_experiments__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PD_EXTERNAL_RATING_EVENTS__CANDIDATE_2__PD_EXPERIMENTS', \n",   
                    "    ['pd_external_rating_events__hkey', 'pd_experiments__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PD_EXTERNAL_RATING_EVENTS__CANDIDATE_3__PD_EXPERIMENTS', \n",   
                    "    ['pd_external_rating_events__hkey', 'pd_experiments__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PD_EXTERNAL_RATING_EVENTS__CANDIDATE_4__PD_EXPERIMENTS', \n",   
                    "    ['pd_external_rating_events__hkey', 'pd_experiments__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PD_INGREDIENTS__EXPERIMENT_ID__PD_EXPERIMENTS', \n",   
                    "    ['pd_ingredients__hkey', 'pd_experiments__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PD_INTERNAL_RATINGS__EXPERIMENT_ID__PD_EXPERIMENTS', \n",   
                    "    ['pd_internal_ratings__hkey', 'pd_experiments__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_BOTTLINGS__BREW__PROD_BREWS', \n",   
                    "    ['prod_bottlings__hkey', 'prod_brews__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_BREWS_BOILINGS__BREW_ID__PROD_BREWS', \n",   
                    "    ['prod_brews_boilings__hkey', 'prod_brews__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_BREWS_INGREDIENT_ADDS', \n",   
                    "    ['prod_brews__hkey', 'prod_ingredient_products__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_BREWS_MASHINGS__BREW_ID__PROD_BREWS', \n",   
                    "    ['prod_brews_mashings__hkey', 'prod_brews__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_BREWS_MASHING_RESTS__BREW_ID__PROD_BREWS', \n",   
                    "    ['prod_brews_mashing_rests__hkey', 'prod_brews__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_BREWS_SPARGINGS__BREW_ID__PROD_BREWS', \n",   
                    "    ['prod_brews_spargings__hkey', 'prod_brews__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_BREWS__BEER__PROD_RECIPES', \n",   
                    "    ['prod_brews__hkey', 'prod_recipes__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_BREWS__BREWER__MA_EMPLOYEES', \n",   
                    "    ['prod_brews__hkey', 'ma_employees__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_INGREDIENT_PRODUCTS__INGREDIENT_ID__PROD_INGREDIENTS', \n",   
                    "    ['prod_ingredient_products__hkey', 'prod_ingredients__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_RECIPES_INSTRUCTIONS_INGREDIENT_ADDS', \n",   
                    "    ['prod_recipes_instructions__hkey', 'prod_ingredients__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_RECIPES_INSTRUCTIONS__BEER_ID__PROD_RECIPES', \n",   
                    "    ['prod_recipes_instructions__hkey', 'prod_recipes__hkey'])\n",   
                    "\n",   
                    "load.create_link(\n",   
                    "    'LNK__PROD_RECIPES__PRODUCT_OWNER__MA_EMPLOYEES', \n",   
                    "    ['prod_recipes__hkey', 'ma_employees__hkey'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__MA_EMPLOYEES__PIT', \n",   
                    "    ['SAT__MA_EMPLOYEES'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PD_EXPERIMENTS__PIT', \n",   
                    "    ['SAT__PD_EXPERIMENTS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PD_EXTERNAL_RATING_EVENTS__PIT', \n",   
                    "    ['SAT__PD_EXTERNAL_RATING_EVENTS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PD_INGREDIENTS__PIT', \n",   
                    "    ['SAT__PD_INGREDIENTS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PD_INTERNAL_RATINGS__PIT', \n",   
                    "    ['SAT__PD_INTERNAL_RATINGS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_BOTTLINGS__PIT', \n",   
                    "    ['SAT__PROD_BOTTLINGS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_BREWS_BOILINGS__PIT', \n",   
                    "    ['SAT__PROD_BREWS_BOILINGS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_BREWS_MASHINGS__PIT', \n",   
                    "    ['SAT__PROD_BREWS_MASHINGS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_BREWS_MASHING_RESTS__PIT', \n",   
                    "    ['SAT__PROD_BREWS_MASHING_RESTS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_BREWS_SPARGINGS__PIT', \n",   
                    "    ['SAT__PROD_BREWS_SPARGINGS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_BREWS__PIT', \n",   
                    "    ['SAT__PROD_BREWS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_INGREDIENTS__PIT', \n",   
                    "    ['SAT__PROD_INGREDIENTS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_INGREDIENT_PRODUCTS__PIT', \n",   
                    "    ['SAT__PROD_INGREDIENT_PRODUCTS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_RECIPES_INSTRUCTIONS__PIT', \n",   
                    "    ['SAT__PROD_RECIPES_INSTRUCTIONS'])\n",   
                    "\n",   
                    "load.create_pit(\n",   
                    "    'SAT__PROD_RECIPES__PIT', \n",   
                    "    ['SAT__PROD_RECIPES'])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__MA_EMPLOYEES', \n",   
                    "    [('firstname', 'STRING'), ('name', 'STRING'), ('date_of_birth', 'STRING'), ('position', 'STRING')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PD_EXPERIMENTS', \n",   
                    "    [('exp_start', 'STRING'), ('exp_end', 'STRING'), ('mashing_start', 'STRING'), ('mashing_rest_1', 'STRING'), ('mashing_rest_1_duration', 'INT'), ('mashing_rest_2', 'STRING'), ('mashing_rest_2_duration', 'INT'), ('mashing_rest_3', 'STRING'), ('mashing_rest_3_duration', 'INT'), ('mashing_rest_4', 'STRING'), ('mashing_rest_4_duration', 'INT'), ('mashing_end', 'STRING'), ('sparging_start', 'STRING'), ('sparging_end', 'STRING'), ('boiling_start', 'STRING'), ('hop_1_adding', 'STRING'), ('hop_2_adding', 'STRING'), ('hop_3_adding', 'STRING'), ('boiling_end', 'STRING'), ('yeast_adding', 'STRING'), ('original_gravity', 'INT'), ('final_gravity', 'INT')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PD_EXTERNAL_RATING_EVENTS', \n",   
                    "    [('candidate_1', 'INT'), ('candidate_1_remaining', 'INT'), ('candidate_1_empty', 'STRING'), ('candidate_2', 'INT'), ('candidate_2_remaining', 'INT'), ('candidate_2_empty', 'STRING'), ('candidate_3', 'INT'), ('candidate_3_remaining', 'INT'), ('candidate_3_empty', 'STRING'), ('candidate_4', 'INT'), ('candidate_4_remaining', 'INT'), ('candidate_4_empty', 'STRING')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PD_INGREDIENTS', \n",   
                    "    [('experiment_id', 'INT'), ('name', 'STRING'), ('description', 'STRING'), ('amount', 'INT'), ('unit', 'STRING')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PD_INTERNAL_RATINGS', \n",   
                    "    [('rating', 'INT')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_BOTTLINGS', \n",   
                    "    [('brew', 'INT'), ('bottled', 'STRING'), ('best_before_date', 'STRING'), ('quantity', 'INT'), ('bottles', 'INT')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_BREWS', \n",   
                    "    [('beer', 'STRING'), ('brewer', 'STRING'), ('brew_start', 'STRING'), ('brew_end', 'STRING'), ('original_gravity', 'INT'), ('final_gravity', 'INT')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_BREWS_BOILINGS', \n",   
                    "    [('end_time', 'STRING')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_BREWS_MASHINGS', \n",   
                    "    [('end_time', 'STRING'), ('start_temperature', 'INT'), ('end_temperature', 'INT')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_BREWS_MASHING_RESTS', \n",   
                    "    [('end_time', 'STRING')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_BREWS_SPARGINGS', \n",   
                    "    [('end_time', 'STRING')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_INGREDIENTS', \n",   
                    "    [('name', 'STRING'), ('unit', 'STRING')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_INGREDIENT_PRODUCTS', \n",   
                    "    [('ingredient_id', 'INT'), ('producer_product_id', 'STRING'), ('producer_name', 'STRING'), ('product_name', 'STRING')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_RECIPES', \n",   
                    "    [('beer_name', 'STRING'), ('product_owner', 'STRING'), ('created', 'STRING'), ('updated', 'STRING')])\n",   
                    "\n",   
                    "load.create_satellite(\n",   
                    "    'SAT__PROD_RECIPES_INSTRUCTIONS', \n",   
                    "    [('beer_id', 'STRING'), ('sort', 'INT')])\n",   
                    "\n",   
                    "#\n",   
                    "# Initialize views\n",   
                    "#\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__MA_EMPLOYEES\n",   
                    "    AS SELECT e.id, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.firstname, s0.name, s0.date_of_birth, s0.position\n",   
                    "    FROM { load.raw_database_name }.HUB__MA_EMPLOYEES AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__MA_EMPLOYEES__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__MA_EMPLOYEES AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.ma_employees\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PD_EXPERIMENTS\n",   
                    "    AS SELECT e.brewer, e.description, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.exp_start, s0.exp_end, s0.mashing_start, s0.mashing_rest_1, s0.mashing_rest_1_duration, s0.mashing_rest_2, s0.mashing_rest_2_duration, s0.mashing_rest_3, s0.mashing_rest_3_duration, s0.mashing_rest_4, s0.mashing_rest_4_duration, s0.mashing_end, s0.sparging_start, s0.sparging_end, s0.boiling_start, s0.hop_1_adding, s0.hop_2_adding, s0.hop_3_adding, s0.boiling_end, s0.yeast_adding, s0.original_gravity, s0.final_gravity\n",   
                    "    FROM { load.raw_database_name }.HUB__PD_EXPERIMENTS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PD_EXPERIMENTS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PD_EXPERIMENTS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.pd_experiments\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PD_EXTERNAL_RATING_EVENTS\n",   
                    "    AS SELECT e.year, e.month, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.candidate_1, s0.candidate_1_remaining, s0.candidate_1_empty, s0.candidate_2, s0.candidate_2_remaining, s0.candidate_2_empty, s0.candidate_3, s0.candidate_3_remaining, s0.candidate_3_empty, s0.candidate_4, s0.candidate_4_remaining, s0.candidate_4_empty\n",   
                    "    FROM { load.raw_database_name }.HUB__PD_EXTERNAL_RATING_EVENTS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PD_EXTERNAL_RATING_EVENTS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PD_EXTERNAL_RATING_EVENTS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.pd_external_rating_events\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PD_INGREDIENTS\n",   
                    "    AS SELECT e.id, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.experiment_id, s0.name, s0.description, s0.amount, s0.unit\n",   
                    "    FROM { load.raw_database_name }.HUB__PD_INGREDIENTS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PD_INGREDIENTS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PD_INGREDIENTS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.pd_ingredients\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PD_INTERNAL_RATINGS\n",   
                    "    AS SELECT e.experiment_id, e.rated_by, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.rating\n",   
                    "    FROM { load.raw_database_name }.HUB__PD_INTERNAL_RATINGS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PD_INTERNAL_RATINGS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PD_INTERNAL_RATINGS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.pd_internal_ratings\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_BOTTLINGS\n",   
                    "    AS SELECT e.id, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.brew, s0.bottled, s0.best_before_date, s0.quantity, s0.bottles\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_BOTTLINGS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BOTTLINGS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BOTTLINGS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_bottlings\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_BREWS\n",   
                    "    AS SELECT e.id, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.beer, s0.brewer, s0.brew_start, s0.brew_end, s0.original_gravity, s0.final_gravity\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_BREWS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_brews\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_BREWS_BOILINGS\n",   
                    "    AS SELECT e.brew_id, e.start_time, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.end_time\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_BREWS_BOILINGS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS_BOILINGS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS_BOILINGS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_brews_boilings\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_BREWS_MASHINGS\n",   
                    "    AS SELECT e.brew_id, e.start_time, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.end_time, s0.start_temperature, s0.end_temperature\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_BREWS_MASHINGS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS_MASHINGS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS_MASHINGS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_brews_mashings\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_BREWS_MASHING_RESTS\n",   
                    "    AS SELECT e.brew_id, e.start_time, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.end_time\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_BREWS_MASHING_RESTS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS_MASHING_RESTS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS_MASHING_RESTS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_brews_mashing_rests\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_BREWS_SPARGINGS\n",   
                    "    AS SELECT e.brew_id, e.start_time, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.end_time\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_BREWS_SPARGINGS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS_SPARGINGS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_BREWS_SPARGINGS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_brews_spargings\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_INGREDIENTS\n",   
                    "    AS SELECT e.id, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.name, s0.unit\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_INGREDIENTS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_INGREDIENTS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_INGREDIENTS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_ingredients\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_INGREDIENT_PRODUCTS\n",   
                    "    AS SELECT e.id, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.ingredient_id, s0.producer_product_id, s0.producer_name, s0.product_name\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_INGREDIENT_PRODUCTS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_INGREDIENT_PRODUCTS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_INGREDIENT_PRODUCTS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_ingredient_products\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_RECIPES\n",   
                    "    AS SELECT e.beer_id, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.beer_name, s0.product_owner, s0.created, s0.updated\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_RECIPES AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_RECIPES__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_RECIPES AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_recipes\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "spark.sql(f\"\"\"\n",   
                    "    CREATE VIEW IF NOT EXISTS { load.raw_database_name }.VIEW__PROD_RECIPES_INSTRUCTIONS\n",   
                    "    AS SELECT e.id, p.{ load.LOAD_DATE }, p.{ load.LOAD_END_DATE }, s0.beer_id, s0.sort\n",   
                    "    FROM { load.raw_database_name }.HUB__PROD_RECIPES_INSTRUCTIONS AS e\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_RECIPES_INSTRUCTIONS__PIT AS p ON p.{ load.HKEY } = e.{ load.HKEY }\n",   
                    "    JOIN { load.raw_database_name }.SAT__PROD_RECIPES_INSTRUCTIONS AS s0 ON s0.{ load.HKEY } = e.{ load.HKEY } AND s0.{ load.LOAD_DATE } = p.prod_recipes_instructions\n",   
                    "    \"\"\")\n",   
                    "\n",   
                    "#\n",   
                    "# Stage tables from source files\n",   
                    "#\n",   
                    "load.stage_table('ma_employees', 'ma_employees.parquet', ['id'])\n",   
                    "load.stage_table('pd_experiments', 'pd_experiments.parquet', ['brewer', 'description'])\n",   
                    "load.stage_table('pd_external_rating_events', 'pd_external_rating_events.parquet', ['year', 'month'])\n",   
                    "load.stage_table('pd_ingredients', 'pd_ingredients.parquet', ['id'])\n",   
                    "load.stage_table('pd_internal_ratings', 'pd_internal_ratings.parquet', ['experiment_id', 'rated_by'])\n",   
                    "load.stage_table('prod_bottlings', 'prod_bottlings.parquet', ['id'])\n",   
                    "load.stage_table('prod_brews', 'prod_brews.parquet', ['id'])\n",   
                    "load.stage_table('prod_brews_boilings', 'prod_brews_boilings.parquet', ['brew_id', 'start_time'])\n",   
                    "load.stage_table('prod_brews_ingredient_adds', 'prod_brews_ingredient_adds.parquet', [])\n",   
                    "load.stage_table('prod_brews_mashing_rests', 'prod_brews_mashing_rests.parquet', ['brew_id', 'start_time'])\n",   
                    "load.stage_table('prod_brews_mashings', 'prod_brews_mashings.parquet', ['brew_id', 'start_time'])\n",   
                    "load.stage_table('prod_brews_spargings', 'prod_brews_spargings.parquet', ['brew_id', 'start_time'])\n",   
                    "load.stage_table('prod_ingredient_products', 'prod_ingredient_products.parquet', ['id'])\n",   
                    "load.stage_table('prod_ingredients', 'prod_ingredients.parquet', ['id'])\n",   
                    "load.stage_table('prod_recipes', 'prod_recipes.parquet', ['beer_id'])\n",   
                    "load.stage_table('prod_recipes_instructions', 'prod_recipes_instructions.parquet', ['id'])\n",   
                    "load.stage_table('prod_recipes_instructions_ingredient_adds', 'prod_recipes_instructions_ingredient_adds.parquet', [])\n",   
                    "\n",   
                    "#\n",   
                    "# Load Hub data\n",   
                    "#\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'ma_employees',  'HUB__MA_EMPLOYEES', ['id'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__MA_EMPLOYEES',\n",   
                    "            ['firstname', 'name', 'date_of_birth', 'position']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'pd_experiments',  'HUB__PD_EXPERIMENTS', ['brewer', 'description'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PD_EXPERIMENTS',\n",   
                    "            ['exp_start', 'exp_end', 'mashing_start', 'mashing_rest_1', 'mashing_rest_1_duration', 'mashing_rest_2', 'mashing_rest_2_duration', 'mashing_rest_3', 'mashing_rest_3_duration', 'mashing_rest_4', 'mashing_rest_4_duration', 'mashing_end', 'sparging_start', 'sparging_end', 'boiling_start', 'hop_1_adding', 'hop_2_adding', 'hop_3_adding', 'boiling_end', 'yeast_adding', 'original_gravity', 'final_gravity']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'pd_external_rating_events',  'HUB__PD_EXTERNAL_RATING_EVENTS', ['year', 'month'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PD_EXTERNAL_RATING_EVENTS',\n",   
                    "            ['candidate_1', 'candidate_1_remaining', 'candidate_1_empty', 'candidate_2', 'candidate_2_remaining', 'candidate_2_empty', 'candidate_3', 'candidate_3_remaining', 'candidate_3_empty', 'candidate_4', 'candidate_4_remaining', 'candidate_4_empty']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'pd_ingredients',  'HUB__PD_INGREDIENTS', ['id'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PD_INGREDIENTS',\n",   
                    "            ['experiment_id', 'name', 'description', 'amount', 'unit']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'pd_internal_ratings',  'HUB__PD_INTERNAL_RATINGS', ['experiment_id', 'rated_by'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PD_INTERNAL_RATINGS',\n",   
                    "            ['rating']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_bottlings',  'HUB__PROD_BOTTLINGS', ['id'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_BOTTLINGS',\n",   
                    "            ['brew', 'bottled', 'best_before_date', 'quantity', 'bottles']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_brews',  'HUB__PROD_BREWS', ['id'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_BREWS',\n",   
                    "            ['beer', 'brewer', 'brew_start', 'brew_end', 'original_gravity', 'final_gravity']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_brews_boilings',  'HUB__PROD_BREWS_BOILINGS', ['brew_id', 'start_time'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_BREWS_BOILINGS',\n",   
                    "            ['end_time']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_brews_mashing_rests',  'HUB__PROD_BREWS_MASHING_RESTS', ['brew_id', 'start_time'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_BREWS_MASHING_RESTS',\n",   
                    "            ['end_time']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_brews_mashings',  'HUB__PROD_BREWS_MASHINGS', ['brew_id', 'start_time'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_BREWS_MASHINGS',\n",   
                    "            ['end_time', 'start_temperature', 'end_temperature']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_brews_spargings',  'HUB__PROD_BREWS_SPARGINGS', ['brew_id', 'start_time'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_BREWS_SPARGINGS',\n",   
                    "            ['end_time']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_ingredient_products',  'HUB__PROD_INGREDIENT_PRODUCTS', ['id'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_INGREDIENT_PRODUCTS',\n",   
                    "            ['ingredient_id', 'producer_product_id', 'producer_name', 'product_name']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_ingredients',  'HUB__PROD_INGREDIENTS', ['id'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_INGREDIENTS',\n",   
                    "            ['name', 'unit']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_recipes',  'HUB__PROD_RECIPES', ['beer_id'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_RECIPES',\n",   
                    "            ['beer_name', 'product_owner', 'created', 'updated']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_hub_from_staging_table(\n",   
                    "    'prod_recipes_instructions',  'HUB__PROD_RECIPES_INSTRUCTIONS', ['id'], \n",   
                    "    [\n",   
                    "        (\n",   
                    "            'SAT__PROD_RECIPES_INSTRUCTIONS',\n",   
                    "            ['beer_id', 'sort']\n",   
                    "        )\n",   
                    "    ])\n",   
                    "\n",   
                    "#\n",   
                    "# Load Link data\n",   
                    "#\n",   
                    "load.load_link_from_link_staging_table(\n",   
                    "    'prod_brews_ingredient_adds', \n",   
                    "    [\n",   
                    "            ['prod_brews', 'brew_id', 'id', 'prod_brews__hkey'], \n",   
                    "            ['prod_ingredient_products', 'ingredient_product', 'id', 'prod_ingredient_products__hkey']\n",   
                    "    ],\n",   
                    "    'LNK__PROD_BREWS_INGREDIENT_ADDS', \n",   
                    "    [\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_link_from_link_staging_table(\n",   
                    "    'prod_recipes_instructions_ingredient_adds', \n",   
                    "    [\n",   
                    "            ['prod_recipes_instructions', 'id', 'id', 'prod_recipes_instructions__hkey'], \n",   
                    "            ['prod_ingredients', 'ingredient', 'id', 'prod_ingredients__hkey']\n",   
                    "    ],\n",   
                    "    'LNK__PROD_RECIPES_INSTRUCTIONS_INGREDIENT_ADDS', \n",   
                    "    [\n",   
                    "    ])\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'pd_external_rating_events', 'pd_experiments',\n",   
                    "    'candidate_1', 'id',\n",   
                    "    'pd_external_rating_events__hkey', 'pd_experiments__hkey',\n",   
                    "    'LNK__PD_EXTERNAL_RATING_EVENTS__CANDIDATE_1__PD_EXPERIMENTS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'pd_external_rating_events', 'pd_experiments',\n",   
                    "    'candidate_2', 'id',\n",   
                    "    'pd_external_rating_events__hkey', 'pd_experiments__hkey',\n",   
                    "    'LNK__PD_EXTERNAL_RATING_EVENTS__CANDIDATE_2__PD_EXPERIMENTS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'pd_external_rating_events', 'pd_experiments',\n",   
                    "    'candidate_3', 'id',\n",   
                    "    'pd_external_rating_events__hkey', 'pd_experiments__hkey',\n",   
                    "    'LNK__PD_EXTERNAL_RATING_EVENTS__CANDIDATE_3__PD_EXPERIMENTS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'pd_external_rating_events', 'pd_experiments',\n",   
                    "    'candidate_4', 'id',\n",   
                    "    'pd_external_rating_events__hkey', 'pd_experiments__hkey',\n",   
                    "    'LNK__PD_EXTERNAL_RATING_EVENTS__CANDIDATE_4__PD_EXPERIMENTS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'pd_ingredients', 'pd_experiments',\n",   
                    "    'experiment_id', 'id',\n",   
                    "    'pd_ingredients__hkey', 'pd_experiments__hkey',\n",   
                    "    'LNK__PD_INGREDIENTS__EXPERIMENT_ID__PD_EXPERIMENTS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'pd_internal_ratings', 'pd_experiments',\n",   
                    "    'experiment_id', 'id',\n",   
                    "    'pd_internal_ratings__hkey', 'pd_experiments__hkey',\n",   
                    "    'LNK__PD_INTERNAL_RATINGS__EXPERIMENT_ID__PD_EXPERIMENTS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_bottlings', 'prod_brews',\n",   
                    "    'brew', 'id',\n",   
                    "    'prod_bottlings__hkey', 'prod_brews__hkey',\n",   
                    "    'LNK__PROD_BOTTLINGS__BREW__PROD_BREWS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_brews', 'ma_employees',\n",   
                    "    'brewer', 'id',\n",   
                    "    'prod_brews__hkey', 'ma_employees__hkey',\n",   
                    "    'LNK__PROD_BREWS__BREWER__MA_EMPLOYEES')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_brews', 'prod_recipes',\n",   
                    "    'beer', 'beer_id',\n",   
                    "    'prod_brews__hkey', 'prod_recipes__hkey',\n",   
                    "    'LNK__PROD_BREWS__BEER__PROD_RECIPES')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_brews_boilings', 'prod_brews',\n",   
                    "    'brew_id', 'id',\n",   
                    "    'prod_brews_boilings__hkey', 'prod_brews__hkey',\n",   
                    "    'LNK__PROD_BREWS_BOILINGS__BREW_ID__PROD_BREWS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_brews_mashing_rests', 'prod_brews',\n",   
                    "    'brew_id', 'id',\n",   
                    "    'prod_brews_mashing_rests__hkey', 'prod_brews__hkey',\n",   
                    "    'LNK__PROD_BREWS_MASHING_RESTS__BREW_ID__PROD_BREWS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_brews_mashings', 'prod_brews',\n",   
                    "    'brew_id', 'id',\n",   
                    "    'prod_brews_mashings__hkey', 'prod_brews__hkey',\n",   
                    "    'LNK__PROD_BREWS_MASHINGS__BREW_ID__PROD_BREWS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_brews_spargings', 'prod_brews',\n",   
                    "    'brew_id', 'id',\n",   
                    "    'prod_brews_spargings__hkey', 'prod_brews__hkey',\n",   
                    "    'LNK__PROD_BREWS_SPARGINGS__BREW_ID__PROD_BREWS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_ingredient_products', 'prod_ingredients',\n",   
                    "    'ingredient_id', 'id',\n",   
                    "    'prod_ingredient_products__hkey', 'prod_ingredients__hkey',\n",   
                    "    'LNK__PROD_INGREDIENT_PRODUCTS__INGREDIENT_ID__PROD_INGREDIENTS')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_recipes', 'ma_employees',\n",   
                    "    'product_owner', 'id',\n",   
                    "    'prod_recipes__hkey', 'ma_employees__hkey',\n",   
                    "    'LNK__PROD_RECIPES__PRODUCT_OWNER__MA_EMPLOYEES')\n",   
                    "\n",   
                    "load.load_link_from_linked_staging_tables(\n",   
                    "    'prod_recipes_instructions', 'prod_recipes',\n",   
                    "    'beer_id', 'beer_id',\n",   
                    "    'prod_recipes_instructions__hkey', 'prod_recipes__hkey',\n",   
                    "    'LNK__PROD_RECIPES_INSTRUCTIONS__BEER_ID__PROD_RECIPES')\n",   
                    "\n",   
                    "#\n",   
                    "# Update PIT tables\n",   
                    "#\n",   
                    "load.update_pit(\n",   
                    "    'SAT__MA_EMPLOYEES__PIT',\n",   
                    "    ['SAT__MA_EMPLOYEES'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PD_EXPERIMENTS__PIT',\n",   
                    "    ['SAT__PD_EXPERIMENTS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PD_EXTERNAL_RATING_EVENTS__PIT',\n",   
                    "    ['SAT__PD_EXTERNAL_RATING_EVENTS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PD_INGREDIENTS__PIT',\n",   
                    "    ['SAT__PD_INGREDIENTS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PD_INTERNAL_RATINGS__PIT',\n",   
                    "    ['SAT__PD_INTERNAL_RATINGS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_BOTTLINGS__PIT',\n",   
                    "    ['SAT__PROD_BOTTLINGS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_BREWS_BOILINGS__PIT',\n",   
                    "    ['SAT__PROD_BREWS_BOILINGS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_BREWS_MASHINGS__PIT',\n",   
                    "    ['SAT__PROD_BREWS_MASHINGS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_BREWS_MASHING_RESTS__PIT',\n",   
                    "    ['SAT__PROD_BREWS_MASHING_RESTS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_BREWS_SPARGINGS__PIT',\n",   
                    "    ['SAT__PROD_BREWS_SPARGINGS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_BREWS__PIT',\n",   
                    "    ['SAT__PROD_BREWS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_INGREDIENTS__PIT',\n",   
                    "    ['SAT__PROD_INGREDIENTS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_INGREDIENT_PRODUCTS__PIT',\n",   
                    "    ['SAT__PROD_INGREDIENT_PRODUCTS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_RECIPES_INSTRUCTIONS__PIT',\n",   
                    "    ['SAT__PROD_RECIPES_INSTRUCTIONS'])\n",   
                    "\n",   
                    "load.update_pit(\n",   
                    "    'SAT__PROD_RECIPES__PIT',\n",   
                    "    ['SAT__PROD_RECIPES'])\n"
                ],
                "execution_count": null
            }
        ]
    }
}